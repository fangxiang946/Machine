{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.特征提取\n",
    " + ### 1.1 字典提取  [DictVectorizer]\n",
    "  + 1.1.1 类别字段会转换成one-hot编码\n",
    "  + 1.1.2 转换后获取的结果可以是二维数组，也可以是稀疏矩阵（默认）    sparse就是稀疏矩阵\n",
    "  + 1.1.3 稀疏矩阵就是将二维数组中的非0数值表示出来，这样可以节省空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "#1.实例一个DictVectorizer对象\n",
    "tranfer = DictVectorizer(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictDic = [{'city':'北京','wendu':1,'detail':'雾霾'},{'city':'珠海','wendu':20,'detail':'大海'},{'city':'本溪','wendu':-5,'detail':'白雪'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.调用提取方法 默认返回sparse矩阵\n",
    "result = tranfer.fit_transform(dictDic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.   0.   0.   0.   0.   1.   1.]\n",
      " [  0.   0.   1.   1.   0.   0.  20.]\n",
      " [  0.   1.   0.   0.   1.   0.  -5.]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['city=北京', 'city=本溪', 'city=珠海', 'detail=大海', 'detail=白雪', 'detail=雾霾', 'wendu']\n"
     ]
    }
   ],
   "source": [
    "print(tranfer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " + ### 1.2 文本提取  [CountVectorizer]\n",
    "  + 1.2.1 英文提取+停用词\n",
    "  + 1.2.1 中文提取+jieba分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 2)\t2\n",
      "  (1, 0)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 1)\t2\n",
      "[[0 0 2 1 0 1]\n",
      " [1 2 0 0 1 0]]\n",
      "['beijing', 'dislike', 'like', 'mother', 'net', 'python']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "stopword=['my']\n",
    "count = CountVectorizer(stop_words=stopword)\n",
    "\n",
    "dictCount = ['I like python,I like my mother','I dislike .net,I dislike beijing']\n",
    "\n",
    "countResult = count.fit_transform(dictCount)\n",
    "print(countResult)\n",
    "print(countResult.toarray())\n",
    "print(count.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "#中文需要自己分词，然后再进行文本提取\n",
    "def cuttext(text):\n",
    "    return \" \".join(list(jieba.cut(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t1\n",
      "  (0, 3)\t2\n",
      "  (0, 2)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 0)\t1\n",
      "[[0 0 1 2 1 0 0 0]\n",
      " [1 1 0 0 0 1 1 1]]\n",
      "['伟大领袖', '前进', '北京', '天安门', '太阳升', '我们', '指引', '毛主席']\n"
     ]
    }
   ],
   "source": [
    "chineseCount = CountVectorizer()\n",
    "\n",
    "dictchineseCount = ['我爱北京天安门,天安门上太阳升','伟大领袖毛主席,指引我们向前进']\n",
    "\n",
    "temp = [] \n",
    "for s in dictchineseCount:\n",
    "    temp.append(cuttext(s))\n",
    "    \n",
    "countchineseResult = chineseCount.fit_transform(temp)\n",
    "print(countchineseResult)\n",
    "print(countchineseResult.toarray())\n",
    "print(chineseCount.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ ### 1.3 文本提取  [TfidfVectorizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.40824829  0.81649658  0.40824829  0.          0.\n",
      "   0.        ]\n",
      " [ 0.4472136   0.4472136   0.          0.          0.          0.4472136\n",
      "   0.4472136   0.4472136 ]]\n",
      "['伟大领袖', '前进', '北京', '天安门', '太阳升', '我们', '指引', '毛主席']\n"
     ]
    }
   ],
   "source": [
    "tf = TfidfVectorizer()\n",
    "\n",
    "textDic = ['我爱北京天安门,天安门上太阳升','伟大领袖毛主席,指引我们向前进']\n",
    "\n",
    "temp = [] \n",
    "for s in textDic:\n",
    "    temp.append(cuttext(s))\n",
    "    \n",
    "result = tf.fit_transform(temp)\n",
    "#print(result)\n",
    "print(result.toarray())\n",
    "print(tf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.特征预处理\n",
    " + ### 2.1 归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tranfer = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textDic = [[99,15,9],\n",
    "           [26,84,5],\n",
    "           [42,62,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = tranfer.fit_transform(textDic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ,  0.5       ],\n",
       "       [ 0.21917808,  0.68115942,  0.        ]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
